# Ollama LLM Provider Configuration
# Copy this file to .env in your project root
#
# Ollama allows you to run large language models locally on your machine.
# Models are downloaded and run locally, providing privacy and no API costs.
#
# Requirements:
# - Install Ollama from https://ollama.ai/
# - Pull the models you want to use: ollama pull <model-name>
# - Start Ollama server: ollama serve (usually runs on http://localhost:11434)

# Set Ollama as the default LLM provider
NOESIUM_LLM_PROVIDER=ollama

# Ollama chat model (default: gemma3:4b)
# Popular chat models (pull with: ollama pull <model-name>):
# - gemma3:4b (Google Gemma 3, 4B parameters, fast)
# - gemma3:8b (Google Gemma 3, 8B parameters, more capable)
# - llama3.1:8b (Meta Llama 3.1, 8B parameters, balanced)
# - llama3.1:70b (Meta Llama 3.1, 70B parameters, most capable)
# - mistral:7b (Mistral 7B, efficient)
# - mixtral:8x7b (Mixtral 8x7B, high quality)
# - phi3:mini (Microsoft Phi-3 Mini, very fast)
# - qwen2.5:7b (Qwen 2.5, 7B parameters)
# See https://ollama.ai/library for all available models
OLLAMA_CHAT_MODEL=gemma3:4b

# Ollama vision model (default: gemma3:4b)
# Vision-capable models:
# - gemma3:4b (multimodal support)
# - gemma3:8b (multimodal support)
# - llama3.2-vision:11b (dedicated vision model)
# - llava:latest (LLaVA vision-language model)
# - qwen2-vl:7b (Qwen2-VL vision model)
# Note: Not all models support vision. Check model documentation.
OLLAMA_VISION_MODEL=gemma3:4b

# Ollama embedding model (default: nomic-embed-text:latest)
# Available embedding models:
# - nomic-embed-text:latest (768 dimensions, general purpose)
# - mxbai-embed-large:latest (1024 dimensions, higher quality)
# - all-minilm:latest (384 dimensions, very fast)
# Pull with: ollama pull <model-name>
OLLAMA_EMBED_MODEL=nomic-embed-text:latest

# Embedding dimensions (default: 768 for nomic-embed-text)
# Adjust based on your embedding model:
# - nomic-embed-text: 768
# - mxbai-embed-large: 1024
# - all-minilm: 384
NOESIUM_EMBEDDING_DIMS=768